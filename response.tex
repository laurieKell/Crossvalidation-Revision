
\iffalse
\section{Resubmission Proceedure}

To revise your manuscript, log into https://mc.manuscriptcentral.com/icesjms and enter your Author Centre, where you will find your manuscript title listed under "Manuscripts with Decisions."  Under "Actions," click on "Create a Revision."  Your manuscript number has been appended to denote a revision.

You will be unable to make your revisions on the originally submitted version of the manuscript.  Instead, revise your manuscript using a word processing program and save it on your computer.  Please also highlight the changes to your manuscript within the document by using the track changes mode in MS Word or by using bold or colored text.

Once the revised manuscript is prepared, you can upload it and submit it through your Author Centre.

When submitting your revised manuscript, you will be able to respond to the comments made by the reviewer(s) in the space provided.  You can use this space to document any changes you make to the original manuscript.  In order to expedite the processing of the revised manuscript, please be as specific as possible in your response to the reviewer(s).

IMPORTANT]  Your original files are available to you when you upload your revised manuscript.  Please delete any redundant files before completing the submission.

Please ensure that the manuscript conforms to the ICES JMS formatting requirements, particularly the resolution and sizing of the Figures (which should be designed to fit the page format of the Journal).

Because we are trying to facilitate timely publication of manuscripts submitted to the ICES Journal of Marine Science, your revised manuscript should be uploaded as soon as possible.  If it is not possible for you to submit your revision in a reasonable amount of time, you may request a short extension of time by emailing the  editorial office.

Once again, thank you for submitting your manuscript to the ICES Journal of Marine Science and I look 



\section*{Review}

\subsection*{Editor's Letter}

Manuscript ID ICESJMS-2020-419 entitled "Validation of stock assessment models. Is it me or my model talking?" which you submitted to the ICES Journal of Marine Science, has been reviewed.  The comments of the reviewers are included at the bottom of this letter. 

The reviewers have recommended publication only after major revisions to your manuscript. Most of the comments are minor but reviewer 1 said that he did not “think the work included in the manuscript is sufficient to support all the conclusions”. Also reviewer 2 has some major comments. My own, minor, comment is that the writing is not always clear and some sentences are not even grammatical. Our editor in chief reminded me that we discourage article titles to contain questions rather than declarative statements. Moreover, the question in the title is not even very clear. 

Therefore, I invite you to respond to the reviewers' comments and revise your manuscript. Please submit your revision by 23-Nov-2020.

\subsection*{Response}

Dear Sarah

Thank you for the review. We were pleased the reviewers found the paper thought provoking. However, they also identified the need for clarification and made suggestions for additional work. We have gone through your recommendations and their comments carefully and restructured the manuscript as suggested, clarified sources of misunderstanding, and deleted sections that were thought to be superfluous. We note that there was no consensus among the reviewers for some of the comments, other than the value of the work. We felt that some of the comments were because we are introducing methods that, although widely used in fields such as energy and climate modelling, are currently not routinely used in fisheries stock assessment.  Where we have not accepted the reviewers recommendations we have provide justification for not doing so. 

Our approach was based on validation and prediction skill. Where prediction skill is a measure of the accuracy of a forecasted value compared to the actual (i.e. observed) value that is not known by the model. There appeared to be some confusion about what was meant by prediction, since the reviewers appeared to have thought that, like in a stock assessment working group this was a two stage process, i.e. first fit to the data then project for management measures. We did not do this, our procedure was similar to a jackknife where we removed points using a tail cut or peel then "predicted" missing values. This means that we did not have to make assumptions about recruitment and catch, and could estimate prediction skill for any data series and observation. Also validation is not a binary process, i.e. valid/invalid, as there is a continuum between the two extremes which requires an iterative process  to examine if a model family should be modified or extended. We were not trying to identify a "best assessment" therefore, but to identify if models were overfitted,  whether it is plausible that the data were generated by a system identical to the model, and if not how they could be extended to better describe the dynamics. We were also not proposing validation as the only diagnostic tool to be used in stock assessment, but as a key tool for use in the stock assessment toolbox. We hopefully have now clarified all these points in our response.

With respect to the missing reference, \cite{brooks2016retrospective} used model based quantities not observations. It was therefore a test of consistency within a model, not validation as they did not use observations to estimate prediction skill, they also conducted a two stage process where additional assumptions had to be made about the future, and relative error. These are important differences which we now explain more clearly in the manuscript.

Regarding the comments about catch advice, this appears to be another misunderstanding, since as part of the hindcast approach the reported catches are used to compute missing observations. We do accept that the impact on catch advice is important, but we argue that this is best conducted using Management Strategy Evaluation. We therefore have another follow up paper that combines the hindcasting approach with MSE \parencite[e.g.][]{pastoors2007validating}.

The reviewer's comment about not supporting "all the conclusions", is perhaps due to a misunderstanding. The reviewers appears to want us to identify a "best assessment". Validation, however, is not a binary process and should be used alongside other methods. Furthermore, we did not want to undermine the work of the IOTC scientific committee, as we recognise that the stock assessment process has to take many factors into account, when agreeing an assessment. Validation is an important, but not the only, tool in this regard particularly as it increases trust in that it is the model talking. We have another paper in review which demonstrates the use of validation as part of the process of identifying a "best assessment" \cite{carvalho2020cookbook}. That paper is provided as a reference and if desired we can send a copy of the paper to support our argument. 

One reviewer states the importance of alternative SS parameterisations and scenarios, while another questions the use of ensembles of multiple models. Using an ensemble (or grid) of models is a common way in the tuna world when providing catch advice and conditioning Operating Models as part of Management Strategy Evaluation. It is also widely used in fields like climate modelling. The reviewer who suggested deleting the reference to ensemble models actually suggests a way of weighting models based on prediction skill. So it appears that it is recognised it is an important research area where model validation can be used. This is something that for example ICES has started to address (see 2021 ASC theme  session M and WKENSEMBLE and WKGMSE3 workshops). This is why we included the discussion of ensemble models and suggest prediction skill as a way of weighting or rejecting models. Again this is a topic that we are currently working on for a follow up paper, and so we think the reference to ensemble models is justified.

We agree with the comments about the need for a simulation exercise, and we are currently designing a study that will address the issues identified by the review. This, however, is beyond the scope of the current paper. This study is a proof of concept and provides a description of how we can evaluate alternative models using prediction skill.

With respect to the subtitle "me or my model talking" this was a homage to the Hodge and Dewar paper, "Is it you or your model talking", but we were afraid of sounding arrogant so changed from the second to the first person. It and our definition of stock assessment were also intended to promote debate. A reviewer stated they liked the subtitle and so we would like to keep it, as it helps support our definition of prediction skill, but have changed the main title to "Validation of stock assessment models using prediction skill" to make the link clearer.
\newpage

\seection{Response to Reviewers}

\subsection*{Reviewer 1}

\subsubsection*{General Comments}

\begin{itemize}
    \item I found this manuscript to be thought provoking and believe that it addresses a worthwhile topic. I also strongly agree with the underlying premise and motivation for this manuscript, namely that the predictive capabilities of assessment models should be given increasing consideration relative to goodness of fit diagnostics. 
    \textit{\newline We agree with this. Thank you!}

        \item I would have liked to have seen a follow up SS run with the length composition removed or down weighted and a demonstration that the validity improves. 
    \textit{\newline We ran an SS scenario where we down weighted the length data (effective sample size was 0.00005) and the results were very similar to the ASPM. However, we don't show this in the paper as we are not developing a best model.}

    \item The ultimate goal of projections in stock assessment is also catch advice. I would suggest a demonstration that a model that is generally valid produces more accurate catch advice. This could be achieved with an expansion of the existing methods to include catch advice in the hindcasting period or through a broader and more general simulation study.
    
    \textit{\newline We could not use the approach of \cite{brooks2016retrospective} as the estimates of model based quantities such as "SSB" can not be used for validation. In a follow on paper we conduct an MSE using a backtesting procedure to address this issue.}

    \item My main concern, however, is that I do not think the manuscript did enough to sufficiently demonstrate the utility or usefulness of the proposed hindcast method. I also do not think the work included in the manuscript is sufficient to support all the conclusions. For example, the SS model was generally found to be “invalid”, which the manuscript attributes to length composition data. But, I can theorize several other reasons for why the SS model might be invalid (e.g., recruitment penalty in the likelihood, overparameterization given that the model is likely estimating thousands of parameters).
    \textit{\newline The intention was not to come up with a "best assessment" but a method that can be used to help validate candidate hypotheses and show where the model should be extended. 
    Validation is not a binary process, i.e. valid/invalid, as there is a continuum between the two extremes which requires an iterative process. We were therefore not trying to identify a "best assessment", but to identify if models were overfitted,  whether it is plausible that the data were generated by a system identical to the model, and if not how they could be extended to better describe the dynamics. We were not proposing validation as the only diagnostic tool.  We are addressing the issues raised by the reviewers comments in other papers e.g. \cite{carvalho2020cookbook}. The ambition of validation is not to prove that a model is correct, but to check that the model cannot be falsified with the available data. This requires extending the model to include relevant hypotheses, collection of appropriate data, and improved knowledge.   For example a trend in a recruit index of abundance could be explained by an increase in recruitment or a reduction in natural mortality, while a trend in an adult index by senescence or a change in selection pattern. We believe that the approach we demonstrate can help. We also did not want to usurp the role of the IOTC scientific committee who have to consider the bigger picture when they agree on an assessment.}

    \item Lastly, I think the manuscript could be much improved by restructuring much of the text, especially between the introduction and methods sections. I expand on these and other topics in the more specific comments below.
    \textit{\newline We have now restructured the model.}

\end{description}

\subsubsection*{Specific Comments}

\begin{description}
\item[Brooks and Legault, 2016] used a similar hindcasting method as that used in this study. I suggest citing that work. Brooks, E.N., and Legault, C.M. 2016. Retrospective forecasting] evaluating performance of stock projections for New England groundfish stocks. CJFAS 73] 935-950. dx.doi.org/10.1139/cjfas2015-0163. 
\textit{\newline Now cited, but Brookes and Legault (2016)  used model based quantities and evaluated stability within a model.  A model can only be validated if it is able to predict observations not used when fitting the model.  As demonstrated here this needs to be based on observations, i.e. is a model free exercise, and creates a platform for evaluation across alternative model structures.}

\item[Line 74-75] Expand on why this can be challenging.
\textit{\newline See next comment.}
\item[Line 120-142] All this could likely be moved to the introduction, and portions of it could be used to resolve the expansion I suggest for lines 74-75.
\textit{\newline Moved to introduction and addresses challenges.}

\item[Line 143-146] I suggest describing the assessment models first, then describe the hindcasting procedure. I think this change would make the methods more consistent with the chronology of the research.
\textit{\newline This has now been done.}

\item[Line 147] It is not clear from this sentence exactly what the main objective is, and so I suggest stating it more explicitly.
\textit{\newline The objective is now clearly stated as "The objective of the paper is validation of models using prediction skill, defined as a measure of the accuracy of a forecast value compared to an  observed value that is not known by the model (i.e. model-free validation). Current literature primarily focuses on the past, and how well models fit these data. Historical performance, however, is no indicator of how well a model may perform in the future."}

\item[Line 156-205] This section switches between describing a base case SS model and providing other background on the fishery and stock status. I suggest moving everything associated with background and stock status to the end of the introduction or give all of this contextual information its own section at the top of the methods. I also assumed that this was a description of the SS model used for the research, but that is never explicitly stated. This lengthy SS description also seemed bizarre here because there is a separate subsection just for Assessment Methods.
\textit{\newline Text has now been restructured, so that 1st assessment methods are described, then the data used for the assessment are summarised as suggested above.}

\item[Line 172-181] When discussing the spatial regions, I suggest using the “R-number” as they appear in Figure 1.
\textit{\newline Done}

\item[Line 190-192] Expand on why the length composition data is used in this way. The length comp data being used this in the SS fit also seems contradictory to the conclusion that this data is causing the SS model to be generally invalid. If length comp is not affecting trends in stock abundance then how is it the source of the validity problems? As in my general comments, I suggest exploring this further.
\textit{\newlineThis is a valid point, since validation can be performed using any data not used in fitting and this includes length composition data. We did attempt validation using length composition data, however, as an aim of the study was to demonstrate how to compare models from different families, and the biomass dynamic model does not use length data. This will be the subject of a further study, i.e. to evaluate integrated models with different structures and data weightings.}

\item[Line 196] need a period after fleets.
\textit{\newline Added}

\item[Line 200] need a period after time.
\textit{\newline Added}

\item[Line 210-216] This can likely be deleted.
\textit{\newline Deleted}

\item[Line 217] I was not sure what this description is an example of.
\textit{\newline It is no longer described as an example but is a description of the ASPM.}

\item[Line 231-235] This paragraph is two sentences that are seemingly unrelated. I suggest revising or deleting.
\textit{\newline Deleted.}

\item[Line 246] The phrase tuned to the base case SS assessment suggests the JABBA model was fit to the biomass estimates from SS. If true, then why? I would have thought JABBA would be fit to the raw catch and index data. Please expand on this topic here.
\textit{\newline This has now been clarified as "priors for the production function parameters were on the SS base case"}

\item[Line 276] The algorithm did not help me at all to understand the methods. It can likely be deleted.
\textit{\newline Deleted}

\item[Line 295] I would suggest using a symbol other than xxx here to more easily distinguish these projected
values from the estimated values of the retrospective analysis in the previous section.
\textit{\newline We have made the definition of MASE more generic, and so have deleted the 2nd equation..}

\item[Line 296] change (1,t) to (1:t).
\textit{\newline Done}

\item[Line 314] MAPE is never defined. I suggest deleting this comparison to MAPE.
\textit{\newline Deleted}

\item[Line 321 and Figure 3] The labels in Figure 3 are “stock” and “harvest”, which here I think are SSB and F. I suggest changing the Figure to be consistent. In the top two rows of Figure 3, the thick black line is described as the estimates from the full time series, but there is a light blue line that extends beyond the thick black line in some of the panels. Shouldn’t the thick black line be the longest time series? I think something is wrong or perhaps I’m misunderstanding. I generally found Figure 3 cumbersome and
difficult to understand, but I do not have any great suggestions for a solution.
\textit{\newline We can not change the legends by row, that is why we used stock and harvest to make them more generic. We have now defined the quantities in the legend. We agree that the figures show a lot of information, and have simplified these now.}

\item[Line 329-331] I wondered here whether any ASPM model would ever have a significant retrospective pattern. So much of the model is fixed and deterministic that it’s hard to imagine a systematic change in the time series estimates as less data are used. I suspect the ASPM would sacrifice fit to the indices as it has no ability to estimate a different biomass trend. This degradation in fit is of course the whole point, but it makes examining the retrospective pattern a bit moot. If my logic here sounds reasonable then perhaps just add a caveat to the discussion.
\textit{\newline This is an important point and is why we used prediction skill based on observations. For example if SS was used as a catch only method \parencite[e.g. Simple SS][]{cope2013implementing} and a retrospective analysis run, then if parameters were fixed or had highly informative priors then there may be no retrospective pattern, even if the model was totally wrong. For example if a catch only method with herring priors was used with data from cod. This is why validation should be done using observations not included in the model. We make this point in the manuscript.}

\item[Line 332] I am not clear on why these methods are considered “model-free”. Everything seems dependent on a model to me. Perhaps either explain further or change the terminology. 
\textit{\newline To clarify we now Define Prediction Skill as "Any measure of the accuracy of a forecasted value compared to the actual (i.e. observed) value that is not known by the model" state that you can only validate models on observations, Then rather than saying model free or model based we refer to "prediction skill"}

\item[Line 335] I think prediction skill was poor for indices 2 and 4, not 2 and 3. Correct?
\textit{\newline Yes, now corrected}

\item[Line 338] This sentence is the only place where the term horizons is used in this context, which made it confusing. Perhaps re-word.
\textit{\newline horizon is now defined before it is used.}

\item[Line 342] The use of length comp is not the only difference between SS and ASPM and so I’m not clear on the logic or intention of this sentence.
\textit{\newline This has now been clarified, i.e. "ASPM is parameterised based on the selectivity estimated by a "full" SS model. The model is then fitted to the abundance indices, assuming a deterministic spawning recruitment relationship, and without the size composition data contributing to the likelihood function."}

\item[Line 343-344] The logic supporting this last sentence is not clear. The preceding sentences just note a few difference between the three models, but no analysis or results are referenced in support of the conclusions in the last sentence. This logic needs to be made much clearer. Also, I wondered here if an implication of the results is that you can have a “valid” model with good prediction skill that is premised on an invalid model. ASPM had the best prediction skill but it uses estimates from the SS model. Does the poor prediction skill of SS also invalidate ASPM given that it depends on SS?
\textit{\newline We have made it clear that the objective of validation is to check that the model can not be falsified with the available data, and validity is a continuum. Therefore we are really saying  how useful a model is in prediction. With respect to ASPM being dependent on SS, we could have looked at alternative ASPM parameterisations, e.g. dome shaped verses flat-topped selection patterns, or different levels of natural mortality. We did not do this as we wanted three "relatively" simple examples that presented a continuum to help demonstrate the concept. We were also not attempting to find a best assessment.}

\item[Line 375] Add a corresponding region number that references Figure 3 for the Eastern Indian Ocean Region.
\textit{\newline Done}

\item[Line 407] I like the “is it me or my model talking” catch phrase, but I think the point is that the somewhat subjective decisions we make in stock assessments might create an invalid model and we need to be checking for these things via prediction skill. That point is not clear here, which makes the catch phrase less impactful. I suggest just expanding on the meaning of the catch phrase.
\textit{\newline We have have now clarified this in the conclusions, i.e. 
As the stock assessment process become more complex, e.g. through the increased use of integrated models, ensembles of models, and Management Strategy Evaluation, there are concerns about a lack of transparency. This is because of the many internal, implicit and often poorly documented assumptions, and a lack of access as only a few highly skilled experts are able to run the models \citep{hilborn2003state}. Therefore to increase confidence in the outputs of a model and trust amongst the public, stake and asset-holders and policymakers it is important for modelers to ask “is it me or my model talking” before others ask the question posed by \cite{hodges1992you} "you or your model talking?".} 

\item[Line 459] The general benefits to doing this type of hindcasting are not sufficiently described anywhere in the manuscript. Is the point to ID lousy data (like in this manuscript), eliminate families of models from consideration, something else? I think the introduction touches on this a little, but it is not clear.
\textit{\newline We have now clarified that the objective is not to prove that a model is correct, but to check that the model can not be falsified with the available data. In other words if a model has poor prediction skill then you either need more informative data or to develop alternative models. This is a step forward from hypothesis testing and model selection which is a way of rejecting rather than extending models. We have made this clearer in the introduction.}

\item[Line 461] I thoroughly enjoyed the fact that this research was put in the context of a conversation with Sidney.
\textit{\newline Thanks}

\item[Figure 2] is never referenced in the body of the manuscript and can be deleted. The column titles “method” and quantity need to be reversed in Table 1.
\textit{\newline Figure 2 has been deleted. Table corrected.}

\end{description}

\newpage
\subsection*{Reviewer 2}OK

\subsubsection*{General}

\begin{itemize}
    \item This paper advocate for the so-called hindcast measures as an alternative to the retrospective measures which are routinely used to validate fish stock assessment models. This is a good idea in its own right. The measures are clearly defined and well reasoned. The authors are rightfully sceptical w.r.t. prediction of model outputs (such as biomasses and fishing mortality measures). The emphasis should be on predicting observed quantities.
    \textit{\newline Thanks for the endorsement.}

    \item Not all assessment models are really capable of predicting into the future based purely on the model used for the assessment - if for instance, the model requires new yearly parameters, then the model itself cannot predict. In that case, forward projections require additional assumptions and in that case, it is debatable if we are validating the model or the additional forecast assumption. This detail deserves to be mentioned.
    \textit{\newline We did not perform projections as a two part process as implied by the reviewer. We omitted observations from the recent period, fitted the model and predicted the missing values.  This is similar to a jackknifecool, th edifference was that we peeled back from the terminal year when deleting observations. A problem with the jackknife, however,  is serial correlation. That is why we  performed a hindcast. %If you can not predict observations how can you validate a model? For example Catch-only-methods that only use catch, if you remove catch then you can not fit that year, so is the method invalid?
    }

    \item In e.g. an age-based assessment there will typically be the different observation uncertainty for different age classes, hence it should be expected that certain age classes will be easier to predict. The current approach uses a raw average of all prediction errors, but that could (and likely will) make the proposed measure dominated by a single age class (e.g. the smallest), which seems unfair. A better measure would penalize small deviations from precise observations more and penalize large deviations from imprecise measurements less.
    \textit{\newline You can use prediction skill for any quantity that is observable and can be predicted by a model. This could be an age class if those data are available. MASE has the desirable properties of scale in-variance so can be used to compare forecasts across data sets with different scales.}

    \item This one I may have misunderstood, but it seems to be implied that contrary to AIC hind-casting can also be used to compare models working on different data series. I don't see for it can be used to compare e.g. a model fitting to a biomass series to a model fitting to an age?
    \textit{\newline AIC can only be used to compare models based on the same input data, this is now stated clearly. }

    \item The idea of using these prediction measures as weights in an ensemble model (or weights in operating models) is mentioned several times but not followed through. That idea does not appear feasible, the paper does not document that it is, and does not provide any details. I would suggest dropping that part of the manuscript.
    \textit{\newline simple prediction skill is widely used elsewhere, we have provided references \parencite[i.e.][]{ianelli2016multi, casanova2009weighting}}. We address this in a follow on paper, so have left the reference to ensemble modelling.}

    \item The basic problem is that these models overlap. So we can imagine two models predict equally well and hence should be given equal weight half-half. Then imagine 10 models] First model is the first of the two models above and the remaining nine models are slight variations of the second model above. Now all 10 predict equally well and hence should each get 1/10 of the weight, but effectively we will end up with giving 90\% weight to the second model and 10\% weight to the first.
   \textit{\newline A main issue is the initial choice of models, we address this in our next paper.}

    \item To solve this problem we should be able to estimate covariances in the prediction errors (to get a first approximation of the overlap), and then weight accordingly to that, but is fisheries time series we are very unlikely to get time-series long enough to estimate such covariance matrices.   
    \textit{\newline The reviewer proposes a way of weighting that could be investigated using the approach in the paper. We used twenty years for the hindcast period, and could calculate the covariance. However, covariances are affected by the scale, so it may be better to use correlations. This is a potential research area, and so we would like to keep the reference to ensemble models in the paper.}
    
    %A key point is that validation is not binary i.e. pass/fail, as there is a continuum between these two extremes. So the objective is not to come up with a best assessment, and several models may pass the check that the model cannot be falsified with the available data. In which case advice needs to take this into account, i.e. Kobe PP \cite{kell2016quantification}.
    
    
    %There are many potential error metrics, and the best statistical measure to use depends on the objectives of the analysis and using more than one measure can be helpful in providing insight into the nature of observation and process error structures \parencite{kell2016xval}.
    
    %For example a commonly used measure is root mean square error. As the square root of a variance it can also be interpreted as the standard deviation of the unexplained variance, lower values indicate better fits. $E^\prime$ is sensitive to outliers, however,and favours forecasts that avoid large deviations from the mean and cannot be used to compare across series.  For example the correlation ($\rho$) between $Y_y$ and $\bar{Y_y}$ is not affected by the amplitude of the variations, is insensitive to biases and errors in variance, and can be used to compare across series. $E^{\prime 2}$ and $\rho$ are related by the cosine, and so could be summarised in a single diagramme, for simplicity, however, we settled on MASE.
    
    %MASE was chosen because it has the desirable properties of scale invariance, predictable behaviour, symmetry, interpretability and asymptotic normality. MASE does not skew its distribution even when the observed values are close to zero. MASE is also easier to interpret as a score of 0.5 indicates that the model forecasts are twice as accurate as a na\''{i}ve baseline prediction; the model thus has prediction skill.

    \item The prediction measure does not take into account how similar or different the models are, and assessment are often very similar (slight variations of the same basic model).
    \textit{\newline The reviewer is correct it does not, and we say nothing about model complexity. However, we do implicitly address overfitting, a problem with models with many parameters.}

    \item (in the text somehow all less than symbol "<" was turned into "!" in my version?)
    \textit{\newline done}

\end{itemize}


\subsubsection*{Specific Comments}

\newpage
\subsection*{Reviewer 3}
\subsubsection*{General}

\begin{description}
    \item Stock assessment models have increased in complexity and may use a wide array of data that needs to be properly weighted in the objective functions used to estimate the parameters, to avoid the risk of biasing the results. Thus, new diagnostic tools are a welcome addition to the toolbox of stock assessment modelers, to help detect model misspecification and appropriate weighting of different data set. The hindcast diagnostic proposed by the author seems interesting as model diagnostic and as a way of ranking models.  I find the manuscript interesting because there is a need for diagnostics for stock assessment models and it is a field that is very active, with several different diagnostics been proposed in recent years. The authors demonstrated that the hindcasting diagnostic detected differences in the prediction ability of different stock assessment models that were not detected by retrospective analyses. The authors conclude that the ASPM is a better model to assess the yellowfin tuna in the Indian Ocean judging by the hindcasting diagnostics. All models had similar retrospective patterns; thus, the hindcasting seems to be superior for differentiating among models. I believe the manuscript would be of interest to the reader of ICES JMS. I recommend the publication of the paper after the authors perform some revisions to improve clarity.
    \textit{\newline Thanks}

    \item The definition of stock assessment put forward by the authors (“The description of the characteristics of a 'stock' so that its biological reaction to being exploited can be rationally predicted and the predictions tested, Holt) seems to be a good operational definition that allows for practical developments such as the proposed of diagnostics. However, I feel that not all stock assessments have the goal of predicting, some have the main goal of estimating the status of the stock in relation to pre-defined reference points while acknowledging that prediction may be an unattainable goal. In the case of the goal not being that of predicting, what would be the advantage of the proposed hindcast diagnostic?
    \textit{\newline In the hindcast we looked at one step and three step ahead predictions by conducting a peel. We did no go beyond the range of the data, so we actually addressed the reviewers concern. We have now made this clear.}

    \item The main goal of the paper and the connection to the title should be stated more clearly at the end of the introduction. In Line 115, the authors state] “as a working example, we compare three model families etc…” what is the connection between the title and the goals of the paper? “When is “me” talking versus the model talking? When the model cannot be used to predict? So if the models do not pass the validation, it is “me” talking? Some of the text at the end of the abstract plus some text from the methods section (Line 147 to 149) can be used to word the main goals of the manuscript by the end of the introduction section.
    \textit{\newline We have added a sentence in the end of the Introduction that covers this comment

    \item It is not completely clear how the hindcasting is computed, in the explanation in the text the authors talk about predictions but in Line 276 where the algorithm is stated, there is no step related to predictions. How are the predictions obtained for each of the three stock assessment models? Are the actual catches by fishery and area for the year that been predicted used? Or the fishing mortality is held at the values from the previous year or average for a group of years? Also, what recruitment deviations by area and time are used for the predicted years? Are those set to zero? Or it is a sample from the previous years? Those details need to be explained more clearly, to be able to assess whether the hindcasting is diagnosing the actual assessment model rather than the projection algorithm.
    \textit{\newline There was no actual "projection", observations were removed by tail cutting and then missing values predicted.}

\end{description}


\subsubsection*{Specific Comments}

\begin{description}
\item[Line 50] instead of “methods” maybe use “diagnostics”
\textit{\newline changed}

\item[Line 51] “requires a reference set of observations” and the models do not share the same data? If they do, a reference set of observations could be 
\textit{\newline MASE can be used to compare different series.}

\item[Line 79] the ‘model-free’ predictions are still derived from models since we need both the dynamic model and the observation model to predict the pseudo-observations, right? This is something like the posterior predictive checks of Bayesian inference. In that sense, those are not model-free predictions. 
\textit{\newline We have clarified this in our definition based on prediction skill.}

\item[Line 103 (and lines  -528)] The correct reference is Hodges \& Dewar 1992 see here https://www.rand.org/pubs/reports/R4114.html
\textit{\newline Changed}

\item[Line 126] Also AIC needs to be performed on models with the same likelihood function (that is with the same data), which are not often the case in stock assessment, where different hypotheses may be modeled as completely different model structures requiring different data sets.
\textit{\newline Agreed and added to text.}

\item[Line 130] A standard diagnostic tool 
\textit{\newline Changed}

\item[Line 147 to 149] A similar sentence needs to be put at the end of the introduction to state the objectives and guide the reader on what to expect in the paper.
\textit{\newline Sentence is now at the end of the introduction.}

\item[Line 210] Recent trend? I don’t think is that recent…. The first stock synthesis assessment is from the 1980s
\textit{\newline This section has now been deleted as suggested by Reviewer 1.}


\item[Line 233] “this is a problem”? what is a problem? The estimation of the production function? Model misspecification? I am not sure what the authors are referring to. 
\textit{\newline Now deleted as proposed by Reviewer 1.}

\item[Line 245] how did the authors implemented a biomass dynamic assessment in SS3? How where the parameters “tuned”? Also did the ASPM and the biomass dynamic model preserve the spatial structure of the SS3 model or where a one area model? 
\textit{\newline For the biomass dynamic model we used JABBA, ASPM preserved the spatial structure but JABBA did not, this is stated under methods.}

\item[Line 276] Box with the algorithm what the letters stand for? especially S and n
\textit{\newline Deleted as proposed by Reviewer 1.}

\item[Line 276] What do you do with the catches for the year that you are hindcasting? Do you use the actual catches? Or do you project using the fishing mortality is recent years/last year?
\textit{\newline We use the actual catches.}

\item[Line 297] Again define n, S, T and t after the equations  
\textit{\newline Done}

\item[Line 306] How is the na\"{i}ve prediction computed?
\textit{\newline Now defined}

\item[Line 307] Typo before the number 1 
\textit{\newline Corrected}

\item[Line 311 and 312] add what are all the components in the equations, these two different MASE should have different names
\textit{\newline There are still mean absolute scaled errors, so the statistucs are MSASE, we have added a subscript to distinguish how they were obtained.}

\item[Line 314] Define MAPE, add equation
\textit{\newline Deleted as suggested by Reviewer 1.}

\item[Line 317] Maybe 1/MASE would be more intuitive
\textit{\newline %MASE is widely used as it has many desirable properties such as scale invariance, predictable behaviour, symmetry, interpretability and asymptotic normality. MASE can also be easily interpreted, as values greater than one indicate that in-sample one-step forecasts from the na\"{i}ve method perform better than the forecast values under consideration, in other words model forecasts are worse than a random walk. Conversely, a MASE score of 0.5 indicates that the model forecasts twice as accurate as a na\"{i}ve baseline prediction; thus the model has prediction skill. 
MASE is an error and big is accepted as being "bad", we would therefore prefer to use MASE as is widely used and accepted.}

\item[Line 317] Typo in na\"{i}ve
\textit{\newline Corrected}

\item[Line 343] What about the recruitment deviations? The estimates of the recruitment deviations is another difference between the SS integrated model and the ASPM, unless you did the ASPM-R which also estimates the recruitment deviations. If so, please add this to the text.
\textit{\newline Added.}
\item[Line 444] The link there seems to be a typo.
\textit{\newline Deleted.}
\item[Line  527 -528] (=Line 103 above).The correct reference is Hodges \& Dewar 1992 see here] https://www.rand.org/pubs/reports/R4114.html
\textit{\newline Corrected}

\end{description}
