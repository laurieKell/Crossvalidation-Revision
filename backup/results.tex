The first step was to conduct a retrospective analysis and the estimates of stock biomass (SSB for ASPM and SS, and biomass for JABBA) and F (instantaneous for ASPM and SS, and rate for JABBA) are shown in Figure 3. The terminal estimates were then projected forward for one and
three years assuming the reported catches and the estimated recruitment from the model with all years included (Figure 3).

The above panels show how the model performs with a one year ahead projection (Figures should say B/BMSY and F/FMSY indicators on the one and 3 year ahead projections). In essence, even with a one year ahead prediction, the base case and Jabba Assessment start diverging from the base case values, this pattern gets
worse in the 3 year retrospective patterns.
ASPM "predicted" values are close to those of the assessment that includes all years. For SS, however, there is a large overestimation of future biomass, and a underestimate of F . For JABBA
the strongest retrospective pattern is seen in F, which is underestimated in the 3 year forward predictions. In addition, JABBA indicates a downward bias in Biomass trends as well.
The residuals from the model fits are shown in Figure 6, 

The results from the model-free hindcasts with one and three year ahead predictions are shown
in Figure 5. CPUE’s from area 2 and 4 perform poorly as in the previous figures for the base
case and JABBA. The ASPM also performs poorly for Area 2 in the 3 year hindcast projections.
Model fits for 3 years ahead for both JABBA and the base case perform poorly. Area 2 also fails
for ASPM in the 3 year projections. As a bulk of the catch comes from Area 1 and 2, Area 2 index
of abundance is important to predict. In addition Area 4 in the eastern part of the Indian Ocean
perform poorly with all models in the three year hindcast exercise.

The fits are summarised in Figures 6 and Figure 6 in the form of Taylor diagrams. Taylor diagrams are mathematical diagrams designed to graphically indicate which of the models of is most realistic (Taylor, K.E. 2001). It is used to quantify the degree of correspondence between the modeled and observed behavior in terms of three statistics: the Pearson correlation
coefficient, the root-mean-square error (RMSE) error, and the standard deviation. 

As evident in the one year projection most models seem to do well for (please check this as 2 and 4 should be off in the 2nd panel and it doesn’t appear so). Also it would be good to put the observed variation
(sigma for the indices on a contour for sigma, so we can see if the predictive capability is within the range of variation). In the one year hindcast, all models other than area 2 indices had a high RMSE, high sigma, and low correlation (left panel). Other models seem to have a high overlap (lower left cluster indicating a high correlation, and low RMSE and low sigma). As we go to a three year hindcast,
all models seem to perform poorly, other than the ASPM. Even, the ASPM, appears to do poorly with the 2nd CPUE. the correlation gets worse for all models for CPUE 1 2, though the RMSE and sigma remains low.

From our analysis the following can be discerned with respect to how well each models with respect to the current status quo in evaluating models, i.e. retrospective analysis. We then present the model free evaluating algorithms with hindcast for one and three year ahead projections. We chose 3 years as that is essentially the time-step between assessments in most tRFMO's. This is a crucial step in assessing which models perform better in model-free performance using what we call "model free cross-validation".

For the standard retrospective analysis,    Figure \ref{fig:retro} and Table \ref{tab:retro} summarise the retrospective analysis, taking 0.2 and -0.15 as the cut off points for accepting an assessment all assessments pass. The strongest retrospective patterns are seen for Stock Synthesis, where F is negatively biased, and although the value of Mohn's $\rho$ is low for SSB a strong pattern is seen with underestimates followed by overestimates. 

Although recent Jabba estimates are unbiased historical estimates of F and Biomass are negatively biased. Jabba estimates are problematic as it appears that even if F<FMSY the stock will decline below BMSY. ASPM estimates appears to have little bias and perform well on the retrospective analysis diagnostic for the time step evaluations chosen.

When performing the Retrospective analysis with projections for F/FMSY and B/BMSY, the following can be discerned; i) Figure \ref{fig:proj} and Table \ref{tab:proj} summarise the retrospective analysis combinded with a three prediction, again taking the cut off as 0.2 to -0.15, both SS and Jabba fail.ASPM appears not to show patterns in the projections and performs the best wrt to these criteria; ii) Survey 2 performs poorly across all models, iii) it is possible that the length compositions add noise to SS, and that area effects (JABBA) are important, as ASPM captures those dynamics.However these is no objective way to choose an assessment based on retrospective analysis as the best model would always be B/MSY = 1.

Model-free cross-validation confirms the relative performance of the models. Figures \ref{fig:hy1} and \ref{fig:hy3} shows that SS performs poorly, possibly because length compositions can only be explained by variations in year-class strengths, and projections predict large increase in biomass.RMSE is difficult to interpret (Table \ref{tab:rmse}), MASE easier (Table \ref{tab:mase}). Based on the Taylor diagram, shows that there is a big difference between model residuals and prediction residuals. Figure \ref{fig:td} summarises the historical model fits survey 4 performs the best, i.e. has high correlation for all models, while survey 3 performs poorly.However, when three step ahead projections are considered (Figure \ref{fig:tdhat} ) survey 4 perform the best for ASPM, although the correlation is reduced. SS has high RMSE, poor correlation and high  variance.  

The importance of this approach presented here is the ability to assess model performance, not on how well we do with the past, but how well we do when we project into the future. The model free evaluation techniques shown here provide a framework to evaluate the alternative models examined here. It also provides a recipe of how we can evaluate alternative model platforms in other areas globally using the hindcasting diagnostic.


