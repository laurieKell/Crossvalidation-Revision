A well-fitting model results in predicted values close to the reference values, i.e. if $Y_t$ is the variable of interest at time $t$ and ${\hat{Y}_t}$ is it's predicted value, then the prediction error is $e_t = Y_t - \hat{Y}_t$ and should be small. The accuracy of the predictions can be assessed by various measures (e.g. Hyndman and Athanasopoulos 2018). One example is the Mean Absolute Error (MAE), which is the mean of the absolute errors and measures how big of an error the forecast will generate on average.
\begin{equation} 
\disp {MAE} =\frac{1}{T} \sum_{i=1}^{T} \left| e_{i} \right|
\end{equation} 
The MAE is scale dependent and on the same scale as the data. A problem with the MAE is however that the relative size of the error is not always obvious. Sometimes it is hard to tell a big error from a small error. To deal with this problem, the MAPE below can be calculated instead (i.e. MAE as a percentage)
\begin{equation} 
\disp {MAPE} = \frac{1}{T} \sum_{t=1}^T 100 \ \left|\frac{e_t}{Y_t}\right|,
\end{equation}
which allows forecasts of different series in different scales to be compared.

Both MAE and MAPE are based on the mean error and so may understate the impact of big, but infrequent, errors. To adjust for large rare errors the Root Mean Square Error (RMSE) squares the errors before calculating their mean i.e.
\begin{equation}
E^{\prime} = \sqrt{\frac{1}{T} \sum_{t=1}^T e_t^2}
\end{equation}
This is the square root of the variance of the residuals (strictly if the mean of the residuals is 0 and practically if that is nearly 0) and indicates how close predicted values are close to the observations. As the square root of a variance it can also be interpreted as the standard deviation of the unexplained variance, and so lower values indicate better fits. Although the RMSE is commonly used when simulation testing assessment models \citep[e.g.][]{horbowy2011comparison}, as described above it does not describe average error alone, is sensitive to outliers, favours forecasts that avoid large deviations from the mean, and cannot be used to compare across series.

The best statistical measure to use depends on the objectives of the analysis and using more than one measure can be helpful in providing insight into the nature of observation and process error structures. For example the correlation ($\rho$) between $Y_y$ and $\bar{Y_y}$ is scale-independent, not affected by the amplitude of the variations, is insensitive to biases and errors in variance, and can be used to compare across series. $E^{\prime 2}$ and $\rho$ are related by the cosine rule i.e.
\begin{equation} 
E^{\prime 2} = \sigma_o^2 + \sigma_f^2 - 2\sigma_o\sigma_f\rho, 
\end{equation}
where the reference set ($o$) are the observations ($Y_t$) not included in the retrospective assessment and the values (f) are their estimates ${\hat{Y}_t}$. This means that $E^\prime$, $\rho$ and $\sigma_f$ can be summarised simultaneously  \citep{taylor2001summarizing}. Taylor diagrams therefore provide a concise statistical summary of how well patterns match each other and are therefore especially useful for evaluating multiple aspects or in gauging the relative skill of different models \citep{griggs2002climate}. We can also compare RMSE and MAE to determine whether the forecast contains large but infrequent errors. The larger the difference between RMSE and MAE the more inconsistent the error size.

A more robust and easier to interpret statistic for evaluating prediction skill is the Mean Absolute Scaled Error (MASE) \citep{hyndman2006another}.
\begin{equation} 
{MASE}=
\frac{\frac{1}{T} \sum_{t=1}^{T} \left|e_{t}\right|}
{\frac{1}{T-1} \sum_{t=1}^{T} \left|Y_{t+1}-Y_{t}\right|} 
\end{equation}
The MASE evaluates a model’s prediction skill relative to a na\ddot{i}ve baseline prediction. A prediction is said to have skill if it improves the model forecast compared to the baseline. A widely used baseline forecast for time series is the persistence algorithm that simply takes the value at the previous time step to predict the expected outcome at the next time step as a na\ddot{i}ve in-sample prediction, i.e. tomorrow will the same as today. 

The MASE has the desirable properties of scale invariance, predictable behaviour, symmetry, interpretability and asymptotic normality. MASE is independent of the scale of the data, so can be used to compare forecasts across data sets with different scales. Behaviour is predictable as $y_{t}\rightarrow 0$] Percentage forecast accuracy measures such as the Mean absolute percentage error (MAPE) rely on division of $y_{t}$, skewing the distribution of the MAPE for values of $y_{t}$ near or equal to 0. This is especially problematic for data sets whose scales do not have a meaningful 0, such as temperature in Celsius or Fahrenheit, and for intermittent demand data sets, where $y_{t}=0$  occurs frequently.

Symmetry since The mean absolute scaled error penalises positive and negative forecast errors equally, and penalises errors in large forecasts and small forecasts equally. MASE can be easily interpreted, as values greater than one indicate that in-sample one-step forecasts from the na\ddot{i}ve method perform better than the forecast values under consideration, in other words model forecasts are worse than a random walk. Conversely, a MASE score of 0.5 indicates that the model forecasts twice as accurate as a na\ddot{i}ve baseline prediction; thus the model has prediction skill. The Diebold-Mariano test for one-step forecasts is used to test the statistical significance of the difference between two sets of forecasts. 

%To perform hypothesis testing with the Diebold-Mariano test statistic, it is desirable for $DM ∼ N ( 0 , 1 )$ $DM\sim N(0,1)$ , where $DM$ is the value of the test statistic. The DM statistic for the MASE has been empirically shown to approximate this distribution, while the mean relative absolute error (MRAE), MAPE and sMAPE do not.

[Rho]
[MSSE]

\iffalse
If $Y_t$ is a variable of interest at time $t$ and \eqn{\hat{Y}_t} is it's predicted value then the prediction error is given by $e_t = Y_t - \hat{Y}_t$. For a series of $T$ observations and predictions The accuracy of the predictions can be compared to the actual value by calculating various measures, such as as the Mean Absolute Error (MAE), which is the mean of the absolute errors and tells us how big of an error we can expect from the forecast on average. \\

\eqn{MAE=\frac{\left|e_t\right|}{T}} \\

A problem with the MAE is that the relative size of the error is not always obvious. Sometimes it is hard to tell a big error from a small error. To deal with this problem, we can compute the MAPE instead, i.e. MAE as a percentage, this allows forecasts of different series in different scales to be compared.\\

\eqn{MAPE = \frac{1}{T} \sum_{t=1}^T 100\, \left|\frac{e_t}{Y_t}\right|}\\

Both MAE and MAPE are based on the mean error and so may understate the impact of big, but infrequent, errors. If we focus too much on the mean, we will be caught off guard by the infrequent big error. To adjust for large rare errors, we calculate the Root Mean Square Error (RMSE). By squaring the errors before we calculate their mean and then taking the square root of the mean, we arrive at a measure of the size of the error that gives more weight to the large but infrequent errors than the mean.\\

\eqn{RMSE = \sqrt{\frac{1}{T} \sum_{t=1}^T e_t^2}} \\

We can also compare RMSE and MAE to determine whether the forecast contains large but infrequent errors. The larger the difference between RMSE and MAE the more inconsistent the error size.
 
Another measure is the Mean Absolute Scaled Error (MASE) \\

\eqn{MASE={\frac {\sum _{t=1}^{T}\left|e_{t}\right|}{{\frac {T}{T-1}}\sum _{t=1}^{T}\left|Y_{t+1}-Y_{t}\right|}}} \\

Which has the desirable properties of scale invariance, predictable behaviour, symmetry, interpretability and asymptotic normality
 
The mean absolute scaled error is independent of the scale of the data, so can be used to compare forecasts across data sets with different scales. Behaviour is predictable as $y_{t}\rightarrow 0$] Percentage forecast accuracy measures such as the Mean absolute percentage error (MAPE) rely on division of $y_{t}$, skewing the distribution of the MAPE for values of $y_{t}$ near or equal to 0. This is especially problematic for data sets whose scales do not have a meaningful 0, such as temperature in Celsius or Fahrenheit, and for intermittent demand data sets, where $y_{t}=0$  occurs frequently.

Symmetry since The mean absolute scaled error penalises positive and negative forecast errors equally, and penalises errors in large forecasts and small forecasts equally. In contrast, the MAPE  fail both of these criteria. The mean absolute scaled error can be easily interpreted, as values greater than one indicate that in-sample one-step forecasts from the naïve method perform better than the forecast values under consideration. The Diebold-Mariano test for one-step forecasts is used to test the statistical significance of the difference between two sets of forecasts. To perform hypothesis testing with the Diebold-Mariano test statistic, it is desirable for $DM ∼ N ( 0 , 1 )$ $DM\sim N(0,1)$ , where $DM$ is the value of the test statistic. The DM statistic for the MASE has been empirically shown to approximate this distribution, while the mean relative absolute error (MRAE), MAPE and sMAPE do not.
 
Another measure is Theil's $U$\\
  
\eqn{U= \sqrt{\frac{1}{T}\\
     \sum_{t=1}^{T-1} \left(\frac{e_{t+1}}{Y_t}\right)^2
     \cdot \left[
    \frac{1}{T} \sum_{t=1}^{T-1} 
        \left(\frac{Y_{t+1} - Y_t}{Y_t}\right)^2 \right]^{-1}}}

The more accurate the forecasts, the lower the value of Theil's $U$,   which has a minimum of 0. This measure can be interpreted as the ratio of the RMSE of the proposed forecasting model to the RMSE of  a na\"ive model which simply predicts $Y_{t+1} = Y_t$ for all $t$. The na\"ive model yields $U = 1$; values less than 1 indicate an  improvement relative to this benchmark and values greater than 1 a deterioration.

Altough the methods have their limitations, they are simple tools for evaluating forecast accuracy that can be used without knowing anything about the forecast except the past values of a forecast.

Just because a forecast has been accurate in the past, however, does not mean it will be accurate in the future. Over fitting may make the forecast less accurate and there is always the possibility of an event occurring that the model cannot anticipate, a black swan event. When this happens, you don’t know how big the error will be. Errors associated with these events are not typical errors, which is what the statistics above try to measure. So, while forecast accuracy can tell us a lot about the past, remember these limitations when using forecasts to predict the future.

\fi